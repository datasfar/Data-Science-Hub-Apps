import streamlit as st
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

class GNB_Model():

    def __init__(self, x, y, df):

        # Separamos los datos para entreno y test
        self.x_train, self.x_test, self.y_train,self.y_test = train_test_split(x, y, test_size=0.3, random_state=42)

        # Inicializar el clasificador GaussianNB
        self.model_gnb = GaussianNB()

        # Entrenar el modelo
        self.model_gnb.fit(self.x_train, self.y_train)

        # Realiza la predicciÃ³n con los datos de entrada del usuario
        self.prediction_gnb = self.model_gnb.predict(df)
        self.prediction_proba_gnb = self.model_gnb.predict_proba(df)

        # Metricas adicionales
        self.accuracy = accuracy_score(self.y_test, self.model_gnb.predict(self.x_test))
        self.confusion = confusion_matrix(self.y_test, self.model_gnb.predict(self.x_test))
        self.classification = classification_report(self.y_test, self.model_gnb.predict(self.x_test))

    
    def model_information(self):

        st.subheader('Algoritmo Garussian NB')
        st.markdown("""
GaussianNB es una implementaciÃ³n del algoritmo Naive Bayes, especÃ­ficamente para datos que siguen una distribuciÃ³n normal (gaussiana). Es uno de los clasificadores mÃ¡s simples y eficientes, especialmente adecuado para problemas de clasificaciÃ³n con datos continuos. 

Claves del funcionamiento de Garussian NB (GNB):

1.- Modelo de Probabilidades: GaussianNB se basa en el teorema de Bayes, que utiliza la probabilidad condicional para hacer predicciones. La fÃ³rmula del teorema de Bayes es:

        """)
        st.latex(r"""P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}""")
        st.markdown("""
        donde:

    - ğ‘ƒ(ğ¶âˆ£ğ‘‹)es la probabilidad posterior de la clase ğ¶ dada la observaciÃ³n ğ‘‹.
    - ğ‘ƒ(ğ‘‹âˆ£ğ¶)es la probabilidad de observar ğ‘‹ dado que la clase es ğ¶.
    - ğ‘ƒ(ğ¶) es la probabilidad previa de la clase ğ¶.
    - ğ‘ƒ(ğ‘‹) es la probabilidad previa de observar ğ‘‹.
        
2.- AsunciÃ³n de Independencia: Una de las caracterÃ­sticas clave del Naive Bayes es la asunciÃ³n de independencia condicional, que supone que las caracterÃ­sticas son independientes entre sÃ­ dado el valor de la clase. Aunque esta asunciÃ³n rara vez es cierta en la prÃ¡ctica, el modelo sigue funcionando bien en muchos casos.

3.- DistribuciÃ³n Gaussiana: En GaussianNB, se asume que las caracterÃ­sticas continÃºas siguen una distribuciÃ³n normal (gaussiana). Esto significa que la probabilidad condicional  ğ‘ƒ(ğ‘‹ğ‘–âˆ£ğ¶) para una caracterÃ­stica ğ‘‹ğ‘– dada la clase ğ¶ se modela como:
        """)
        st.latex(r"""P(X_i|C) = \frac{1}{\sqrt{2\pi\sigma_C^2}} \exp\left(-\frac{(X_i - \mu_C)^2}{2\sigma_C^2}\right)""")
        st.markdown("""
donde ğœ‡ğ¶ y ğœğ¶ son la media y la desviaciÃ³n estÃ¡ndar de la caracterÃ­stica ğ‘‹ğ‘– para la clase ğ¶.
        """)
        
        st.subheader("Ventajas:")
        st.markdown("""
- Eficiencia Computacional: GaussianNB es extremadamente rÃ¡pido en el entrenamiento y la predicciÃ³n, ya que solo requiere calcular estadÃ­sticas simples (media y desviaciÃ³n estÃ¡ndar) para cada caracterÃ­stica y clase.
- Pocos ParÃ¡metros a Ajustar: No requiere muchos hiperparÃ¡metros, lo que simplifica el proceso de entrenamiento y ajuste.
- Rendimiento en Datos PequeÃ±os: Funciona bien incluso con conjuntos de datos pequeÃ±os, donde otros algoritmos mÃ¡s complejos podrÃ­an sobreajustar.
- Simplicidad: Es fÃ¡cil de implementar y entender, lo que lo hace adecuado para problemas rÃ¡pidos y prototipado.
        """)

        st.subheader("Limitaciones:")
        st.markdown("""
- AsunciÃ³n de Independencia: La suposiciÃ³n de que las caracterÃ­sticas son independientes puede no ser vÃ¡lida en muchos problemas reales, lo que puede afectar negativamente al rendimiento.
- Sensibilidad a la DistribuciÃ³n de Datos: Si los datos no siguen una distribuciÃ³n normal, el rendimiento de GaussianNB puede verse afectado.
        """)
    
        st.subheader('AplicaciÃ³n en cÃ³digo')
        st.code("""
# Importamos las librerias
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn import datasets
        """)

        st.code("""
# Cargamos el dataset y dividimos los datos deseados
lirio = datasets.load_iris()
x = lirio.data
y = lirio.target
        """)

        st.code("""
# Separamos los datos para entreno y test
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

# Inicializar el clasificador GaussianNB
model_gnb = GaussianNB()

# Entrenar el modelo
model_gnb.fit(x_train, y_train)
        """)
        
        st.code("""
# Generamos los datos a predecir
df = {
        'sepal_length': 6.4,
        'sepal_width': 3.5,
        'petal_length': 6.4,
        'petal_width': 3.2
    }
                
# Realiza la predicciÃ³n con los datos de entrada del usuario
prediction_gnb = model_gnb.predict(df)
prediction_proba_gnb = model_gnb.predict_proba(df)
        """)
        
        st.subheader('PredicciÃ³n con GarussianNB')

    def model_graphics(self, lirio):

        # Mostrar la predicciÃ³n
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Setosa:", f"{round(self.prediction_proba_gnb[0,0], 5)}")
        with col2:
            st.metric("Vesicolor:", f"{round(self.prediction_proba_gnb[0,1], 5)}")
        with col3:
            st.metric("Virginica:", f"{round(self.prediction_proba_gnb[0,2], 5)}")


        # GrÃ¡fico de la predicciÃ³n probabilÃ­stica
        fig, ax = plt.subplots()
        ax.bar(lirio.target_names, self.prediction_proba_gnb[0], color=['blue', 'green', 'red'])
        ax.set_ylabel('Probabilidad')
        ax.set_title('Probabilidad de cada tipo de lirio')
        st.pyplot(fig)




  